{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8923d736-6ba9-4c7b-8e28-f42256cb97a5",
   "metadata": {},
   "source": [
    "### Made in collaboration with Dylan Senescall for NSSSP, 2025\n",
    "Changes: \\\n",
    "2025-07-15 - Adding looping through different `n` values, as discussed, and distribution plotting \\\n",
    "2025-07-22 - Changed randomization process, draws from entire dataframe, uses groupby and apply. Faster! \\\n",
    "2025-08-01 - Added regression analysis to look at how variance in correlations changes as a function of age \\\n",
    "2025-08-28 - Some clean up; no longer tracking changes here (see https://github.com/jtmiles/GRID_lab_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71049a3d-ecc8-4f26-a195-f37ed9c5ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from itertools import starmap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy import signal\n",
    "from functools import partial\n",
    "\n",
    "# This code below doesn't do anything important, \n",
    "# I just think it makes plots look nicer\n",
    "from matplotlib.font_manager import findfont, FontProperties\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"font.serif\"] = ['Georgia', 'Times New Roman',\n",
    "                               'Times', 'Cambria']\n",
    "font = findfont(FontProperties(family=['serif']))\n",
    "mpl.rcParams['mathtext.fontset'] = 'custom'\n",
    "mpl.rcParams['mathtext.rm'] = 'Georgia'\n",
    "mpl.rcParams['mathtext.it'] = 'Georgia:italic'\n",
    "mpl.rcParams['mathtext.bf'] = 'Georgia:bold'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02772fc8-5432-4605-adea-bf9841d599b3",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "\n",
    "You'll have to tell the computer where to look for data\n",
    "The address that directs the computer to a file is called a path\n",
    "\n",
    "example path (from a Windows computer):\n",
    "\"C:\\Users\\jmile3\\OneDrive - SCH\\resting_state\\corrected_spectra.csv\"\n",
    "\n",
    "On Windows, you can right click a file and \"Copy as path\" will show\n",
    "up as an option. Not sure how to do that on Mac but might be worth\n",
    "figuring out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66b70be-f437-4371-9307-a0b648b86bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>region</th>\n",
       "      <th>time</th>\n",
       "      <th>ix</th>\n",
       "      <th>PAF</th>\n",
       "      <th>PAA</th>\n",
       "      <th>INT</th>\n",
       "      <th>offset</th>\n",
       "      <th>slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86b2be</td>\n",
       "      <td>3.42</td>\n",
       "      <td>Middle_frontal_gyrus</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.362499</td>\n",
       "      <td>66.228450</td>\n",
       "      <td>-0.758031</td>\n",
       "      <td>1.676881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86b2be</td>\n",
       "      <td>3.42</td>\n",
       "      <td>Middle_frontal_gyrus</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.223814</td>\n",
       "      <td>43.413084</td>\n",
       "      <td>-0.718833</td>\n",
       "      <td>1.851069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86b2be</td>\n",
       "      <td>3.42</td>\n",
       "      <td>Middle_frontal_gyrus</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.164965</td>\n",
       "      <td>35.571238</td>\n",
       "      <td>-0.783387</td>\n",
       "      <td>1.922896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86b2be</td>\n",
       "      <td>3.42</td>\n",
       "      <td>Middle_frontal_gyrus</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.052414</td>\n",
       "      <td>-0.835454</td>\n",
       "      <td>2.111515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86b2be</td>\n",
       "      <td>3.42</td>\n",
       "      <td>Middle_frontal_gyrus</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.212618</td>\n",
       "      <td>-0.879539</td>\n",
       "      <td>2.211396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID   age                region  time  ix  PAF       PAA        INT  \\\n",
       "0  86b2be  3.42  Middle_frontal_gyrus  0.50   0  3.5  0.362499  66.228450   \n",
       "1  86b2be  3.42  Middle_frontal_gyrus  0.75   1  3.5  0.223814  43.413084   \n",
       "2  86b2be  3.42  Middle_frontal_gyrus  1.00   2  3.5  0.164965  35.571238   \n",
       "3  86b2be  3.42  Middle_frontal_gyrus  1.25   3  NaN       NaN  27.052414   \n",
       "4  86b2be  3.42  Middle_frontal_gyrus  1.50   4  NaN       NaN  24.212618   \n",
       "\n",
       "     offset     slope  \n",
       "0 -0.758031  1.676881  \n",
       "1 -0.718833  1.851069  \n",
       "2 -0.783387  1.922896  \n",
       "3 -0.835454  2.111515  \n",
       "4 -0.879539  2.211396  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data from tables\n",
    "# variables of interest (e.g., PAF, INT, variables that parameterize aperiodic activity, organizing metadata, etc.)\n",
    "\n",
    "params_path = r\"INSERT FOLDER LOCATION WITH 'params_table.csv' HERE\"\n",
    "params_path = r\"C:\\Users\\jmile3\\OneDrive - SCH\\resting_state\\params_table.csv\"\n",
    "params_table = pd.read_csv(params_path,dtype={\"age\":float})\n",
    "\n",
    "# initialize a random number generator for use later\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# increase number below to see more values in the table\n",
    "display(params_table.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47e421a-3edf-4a2d-9172-89c672c92d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['86b2be' 'd419f2' '4ac2b6' '71944e' 'b387f2' '854490' '9d10c8' 'a9952e'\n",
      " 'a29aee' '979eab' '693ffd' 'fca96e' 'ecb43e']\n"
     ]
    }
   ],
   "source": [
    "print(params_table.ID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef371d8-3803-4a6e-9321-1080ff946fc8",
   "metadata": {},
   "source": [
    "## Doing some filtering\n",
    "The code below filters data so that it is a bit nicer to work with from a statistics/math standpoint. Primarily, this means:\n",
    "1. getting rid of data that weren't fit well\n",
    "2. applying a Gaussian-weighted average to the different timeseries of data (which are the columns of the matrix in the way the table is set up here)\n",
    "3. adding an annotation for where in the hierarchy the data sit (S (sensory) or A (association)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9793efa-8580-432a-abea-c4ac5e2e5c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'fit0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7148\\1382086100.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get rid of poor model fits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# params_table[\"keep\"] = np.squeeze(np.abs(resid_table.loc[:,['0.5']])<0.2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mparams_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"keep\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparams_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPAA\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparams_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mparams_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PAF'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'INT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# making a copy of the original data for modification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\python\\envs\\iEEG_analysis\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'fit0'"
     ]
    }
   ],
   "source": [
    "# get rid of poor model fits\n",
    "# params_table[\"keep\"] = np.squeeze(np.abs(resid_table.loc[:,['0.5']])<0.2)\n",
    "params_table[\"keep\"] = (params_table.PAA>0.1) & (params_table.fit0.abs()<0.1)\n",
    "params_table.loc[np.logical_not(params_table.keep),['PAF','INT']] = np.nan\n",
    "\n",
    "# making a copy of the original data for modification\n",
    "df = params_table.copy(deep=True)\n",
    "\n",
    "# smooth with a small-window Gaussian weighted average\n",
    "win = 9 # window is centered, so odd number is recommended\n",
    "std = 1 # dictates window width i.e., influence of surrounding points\n",
    "# group data by these variables - unique combinations will form unique groupings\n",
    "grpvars = [\"ID\",\"age\",\"region\"]\n",
    "df = df.groupby(by=grpvars,sort=False).rolling(window=win,center=True, win_type=\"gaussian\", min_periods=1).mean(win,std).reset_index()\n",
    "\n",
    "# this just gets rid of an extra indexing/organizing column\n",
    "# should always work (so try/except probably not necessary)\n",
    "try:\n",
    "    df = df.drop(columns=\"level_3\")\n",
    "except:\n",
    "    pass\n",
    "# window fuction gets applied to all columns, but don't want these to change\n",
    "# replace with the original data\n",
    "df[[\"keep\",\"time\"]] = params_table[[\"keep\",\"time\"]]\n",
    "\n",
    "\n",
    "# sensorimotor and unimodal cortex vs multi/transmodal association cortex\n",
    "unimod = [\"Postcentral_gyrus\",\"Precentral_gyrus\",\"Superior_temporal_gyrus\"] # sensorimotor/unimodal regions\n",
    "transmod = [\"Middle_frontal_gyrus\", \"Middle_temporal_gyrus\", \"Supramarginal_gyrus\"] # association/transmodal regions\n",
    "df.loc[df.region.isin(unimod),'ctx'] = \"S\" # apparently switching to S-A axis notation...\n",
    "df.loc[df.region.isin(transmod),'ctx'] = \"A\"\n",
    "\n",
    "regions = df.region.unique()\n",
    "print(\"Regions in dataset:\")\n",
    "for reg in regions:\n",
    "    print(\"    \",reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7f7e8-c126-4f37-8b9e-db5364c6ed01",
   "metadata": {},
   "source": [
    "## Defining some helper functions\n",
    "Functions are useful for doing operations that you expect will need to be done over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c675dd-9403-4a47-9019-d3535fcf2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def n_unique(seq):\n",
    "    # nunique method will exist with rolling in pandas 3.0\n",
    "    return(len(seq[seq.notna()].unique()))\n",
    "\n",
    "def genvalidseqs(in_df,cols,n,n_valid,min_diff,min_uni):\n",
    "    '''\n",
    "    find all valid n-length sequences/ranges for a given df and column\n",
    "    \n",
    "    valid sequences/ranges are defined by:\n",
    "        1) having at least n_valid non-nan elements in an n-length window\n",
    "        2) having a range of at least min_diff (window max-min >= min_diff)\n",
    "        3) having at least min_uni unique elements in the window\n",
    "\n",
    "    returns a list of valid end indices that meet the above criteria\n",
    "    \n",
    "    '''\n",
    "    seq_series = in_df[cols]\n",
    "\n",
    "    # calculate the rolling sum of values that are not nan\n",
    "    countnas = seq_series.notna().rolling(window=n,center=False,min_periods=int(n)).sum()\n",
    "    # create a boolean array of values that are above a threshold across cols\n",
    "    valid_counts = (countnas>=n_valid).all(axis=1)\n",
    "    \n",
    "    # calculate the range of values in the window\n",
    "    maxs = seq_series.rolling(window=n,center=False,min_periods=2).max(skipna=True)\n",
    "    # pandas is telling me it can't use skipna for min? maybe if I update to 2.3...\n",
    "    mins = seq_series.rolling(window=n,center=False,min_periods=2).min(numeric_only=True)\n",
    "    # create a boolean array of values that are above a threshold\n",
    "    valid_ranges = ((maxs-mins)>=min_diff).all(axis=1)\n",
    "    \n",
    "    # calculate the number of unique values in the window\n",
    "    num_diffs = seq_series.rolling(window=n,center=False,min_periods=min_uni).apply(n_unique)\n",
    "    # create a boolean array of values that are above a threshold\n",
    "    valid_unis = (num_diffs>=min_uni).all(axis=1)\n",
    "    # union of boolean masks as array of indices\n",
    "    valid_ends = in_df.index[valid_counts & valid_ranges & valid_unis & in_df.keep].to_numpy() \n",
    "    # valid_ends will be used as ending value in range, need to add 1?\n",
    "    # prevent negative values\n",
    "    return valid_ends[valid_ends>=(min(in_df.index)+n)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34376d-f085-40c8-a2f4-3c3cd2f749dd",
   "metadata": {},
   "source": [
    "Your goal is to set the values below:\n",
    "```\n",
    "n\n",
    "min_diff\n",
    "min_uni\n",
    "```\n",
    "\n",
    "`n` is the number of consecutive indices needed in a sequence for calculating a correlation\n",
    "`min_diff` is the range that values should span to count as a valid sequence\n",
    "`min_uni` is the minimum number of unique values that should occur for a sequence to be considered valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fddd4ed-176c-4479-bad2-ed0d8543846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# figure out these values !!!\n",
    "n = 960\n",
    "min_diff = 2\n",
    "min_uni = 5\n",
    "##################\n",
    "cols = [\"PAF\",\"INT\"]\n",
    "\n",
    "# proportion of values that cannot be nan\n",
    "n_valid = int(np.floor(n/2))\n",
    "\n",
    "# randomly pick an ID\n",
    "ID = rng.choice(df.ID.unique())\n",
    "print(ID)\n",
    "# filter table so you're just looking at data from\n",
    "# randomly chosen ID\n",
    "subdf = df.loc[df.ID == ID,:].reset_index(drop=True) \n",
    "# randomly pick a region from that ID\n",
    "reg = rng.choice(subdf.region)\n",
    "print(reg)\n",
    "# now just get data from that ID's region\n",
    "reg_df = subdf.loc[subdf.region == reg,:].reset_index(drop=True)\n",
    "\n",
    "endixs = genvalidseqs(reg_df,cols,n,n_valid,min_diff,min_uni)\n",
    "print(len(endixs))\n",
    "\n",
    "all_ixs = list(starmap(np.arange,np.array((endixs-n,endixs)).T))\n",
    "\n",
    "##########\n",
    "# plotting\n",
    "##########\n",
    "\n",
    "# randomly grab one of the possible valid indices\n",
    "plot_ixs = np.array(all_ixs[rng.integers(0,high=len(all_ixs))])\n",
    "\n",
    "# generate a figure\n",
    "fig,ax = plt.subplots(figsize=(6,3),tight_layout=True)\n",
    "    \n",
    "ax.plot(reg_df.time[plot_ixs],reg_df.PAF[plot_ixs])\n",
    "ax.set_ylim([3, 14])\n",
    "ax.set_ylabel(\"PAF (Hz)\")\n",
    "ax.set_xlabel(\"Time (sec)\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(reg_df.time[plot_ixs],reg_df.INT[plot_ixs],\"r:\")\n",
    "ax2.set_ylim([np.percentile(reg_df.INT[reg_df.INT.notna()],0.5), \n",
    "              np.percentile(reg_df.INT[reg_df.INT.notna()],99.5)])\n",
    "ax2.set_ylabel(\"INT (ms)\",color=\"red\")\n",
    "ax2.set_xlabel(\"Time (sec)\")\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# NOTE:\n",
    "# y limits are set based on percentiles\n",
    "# (can't zoom out much, set to contain 99% of data)\n",
    "# can also set them manually (PAF can be between 3 and 14, INT around 10 to 100)\n",
    "\n",
    "# OTHER NOTE:\n",
    "# might see gaps in data - these are periods when there are no valid data\n",
    "# this almost always means no alpha oscillation was detectable\n",
    "\n",
    "# have to mask out nans to do correlations, this excludes across df index (row)\n",
    "nanmask = reg_df.loc[plot_ixs,[\"PAF\",\"INT\"]].notna().all(axis=1)\n",
    "corr = np.round(stats.pearsonr(reg_df.INT[plot_ixs[nanmask]],reg_df.PAF[plot_ixs[nanmask]]).statistic,3)\n",
    "\n",
    "ax2.annotate(\"$R$ = \"+str(corr),xy=(0.05, 1.025), xycoords=\"axes fraction\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb71ac0d-f988-4f64-8464-c4e810d2bb71",
   "metadata": {},
   "source": [
    "## Looping through different durations of data\n",
    "\n",
    "For simplicity, the cell below is the same as the one above, but with the important part inclosed in a `for loop`. <br>\n",
    "The value of `n` changes each loop *iteration*, and calculates correlations for `iters` random selections of data. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febfefb-4940-420d-b7a9-c133e0895a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# randomly sample and plot distribution of correlations\n",
    "min_diff = 2\n",
    "min_uni = 5\n",
    "# looping over this list of values for n\n",
    "n_list = [40, 80, 120, 160]\n",
    "iters = 1000 \n",
    "dfcols = [\"PAF\",\"INT\"]\n",
    "\n",
    "# generate a figure\n",
    "fig,ax = plt.subplots(nrows=1,ncols=len(n_list),figsize=(6,2),tight_layout=True)\n",
    "# keep track of plotting locations\n",
    "row = 0\n",
    "col = 0\n",
    "\n",
    "for n in n_list:\n",
    "    print(n)\n",
    "    # create a container to store values in\n",
    "    n_corrs = [] # empty list\n",
    "    # Partial function to add multiple args\n",
    "    # don't supply the first arg (df_in) b/c that will be the df\n",
    "    # have to do each iteration b/c n changes\n",
    "    n_valid = int(np.floor(n/2))\n",
    "    \n",
    "    partial_func = partial(genvalidseqs,cols=dfcols,n=n,n_valid=n_valid,min_diff=min_diff,min_uni=min_uni)\n",
    "    # taking a really long time to calculate on whole df...\n",
    "    all_ends = df.loc[df.ID!='71944e',:].groupby(by=[\"ID\",\"region\"]).apply(partial_func, include_groups=False)    \n",
    "    # randomly pick an ID and corresponding reg\n",
    "    # multi-index tuples for use in groupby object\n",
    "    randixs = rng.choice(all_ends.index,[iters,1])\n",
    "    corrs = []\n",
    "    \n",
    "    for tup in randixs:\n",
    "        endixs = np.squeeze(all_ends[tup])\n",
    "        # randomly pick one of the valid sequences\n",
    "        randix = rng.choice(endixs)\n",
    "        use_ixs = np.arange(randix-n,randix)+1\n",
    "        # mask out nans\n",
    "        nanmask = df.loc[use_ixs,[\"PAF\",\"INT\"]].notna().all(axis=1)\n",
    "        \n",
    "        INTs = df.INT[use_ixs[nanmask]]\n",
    "        PAFs = df.PAF[use_ixs[nanmask]]\n",
    "        assert(np.sum(nanmask)>=n_valid)\n",
    "        assert(len(df.region[use_ixs[nanmask]].unique())==1)\n",
    "        \n",
    "        # calculate correlation\n",
    "        corrs.append(stats.pearsonr(df.INT[use_ixs[nanmask]],df.PAF[use_ixs[nanmask]]).statistic)\n",
    "\n",
    "    ax[col].hist(corrs,bins=np.linspace(-1,1,51),histtype='stepfilled',ec=[0.2,0.4,0.2,0.8],fc=[0.2,0.4,0.2,0.4])\n",
    "    ax[col].plot([0,0],[0,iters/10],\"k:\")\n",
    "    ax[col].set_title(\"n = \"+str(n))\n",
    "    col = col+1\n",
    "# show the plots (at the end - out of the loop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac65a5-e0ab-40f6-8d82-1a499537e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_diff = 2\n",
    "min_uni = 5\n",
    "n = int(4*30)\n",
    "n_valid = int(n/2)\n",
    "dfcols = [\"PAF\",\"INT\"]\n",
    "iters = 1000 \n",
    "\n",
    "partial_func = partial(genvalidseqs,cols=dfcols,n=n,n_valid=n_valid,min_diff=min_diff,min_uni=min_uni)\n",
    "all_ends = df.loc[(df.ID!='71944e'),:].groupby(by=[\"ID\",\"region\"]).apply(partial_func, include_groups=False)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef55eb-3a41-4a5b-84ab-3038b328325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a figure\n",
    "p_IDs = df.ID.unique()\n",
    "p_IDs = p_IDs[p_IDs!='71944e']\n",
    "\n",
    "plt.figure(figsize=(6,6),tight_layout=True)\n",
    "corr_dict = {}\n",
    "for ii,ID in enumerate(p_IDs):\n",
    "    # create a container to store values in\n",
    "    IDcorrs = [] # empty list\n",
    "    # generates randomly ordered set of regions (for ID)\n",
    "    randreg = np.squeeze(rng.choice(all_ends[ID].index,[iters,1]))\n",
    "    corrs = []\n",
    "    \n",
    "    for reg in randreg:\n",
    "        # randomly pick one of the valid sequences\n",
    "        # from given (ID, reg) combo\n",
    "        randix = rng.choice(all_ends[(ID,reg)])\n",
    "        ####\n",
    "        # NOTE:\n",
    "        # indices here are from the *original*, full-dateset df\n",
    "        ####\n",
    "        use_ixs = np.arange(randix-n,randix)+1\n",
    "        # mask out nans\n",
    "        nanmask = df.loc[use_ixs,[\"PAF\",\"INT\"]].notna().all(axis=1)\n",
    "        \n",
    "        INTs = df.INT[use_ixs[nanmask]]\n",
    "        PAFs = df.PAF[use_ixs[nanmask]]\n",
    "        # assert(np.sum(nanmask)>=n_valid)\n",
    "        # assert(len(df.region[use_ixs[nanmask]].unique())==1)\n",
    "        \n",
    "        # calculate correlation\n",
    "        corrs.append(stats.pearsonr(df.INT[use_ixs[nanmask]],df.PAF[use_ixs[nanmask]]).statistic)\n",
    "        \n",
    "    # store corr distributions, labeled by ID (in dictionary)\n",
    "    corr_dict[ID] = corrs\n",
    "    ax = plt.subplot(3,4,ii+1)\n",
    "    ax.hist(corrs,bins=np.linspace(-1,1,51), weights=1/len(corrs)*np.ones(len(corrs)),\n",
    "            histtype='stepfilled',ec=[0.2,0.4,0.2,0.8],fc=[0.2,0.4,0.2,0.4])\n",
    "    ax.plot([0,0],[0,0.2],\"k:\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
